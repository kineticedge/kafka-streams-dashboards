from llama_cpp import Llama

# Load your model (adjust path as needed)
llm = Llama(model_path="deepseek-r1.gguf")

# Your token array from the model response
context_tokens = [151669, 2610, 525, 264, 3349, 10519, 17847, 13, 1752, 264, 330, 19552, 7134, 1298, 220, 16, 19, 1, 56125, 481, 386, 18, 1298, 16392, 33054, 369, 8162, 21392, 481, 220, 16, 23, 5381, 13850, 481, 220, 16, 23, 23460, 22670, 481, 220, 16, 31160, 36362, 698, 374, 279, 3349, 315, 400, 16, 11, 19, 24, 24, 13, 24, 24, 5394, 5267, 421, 432, 4977, 18511, 476, 15114, 11, 5889, 1172, 448, 330, 6555, 5868, 15114, 22956, 1416, 279, 3349, 4977, 4622, 11, 5889, 448, 330, 6555, 5868, 4622, 22956, 13655, 697, 2033, 6896, 311, 1493, 31747, 1172, 624, 151670, 151667, 198, 32313, 11, 1077, 594, 1438, 419, 1495, 13, 576, 1196, 374, 10161, 911, 264, 57579, 1298, 220, 16, 19, 1, 448, 458, 386, 18, 1298, 16392, 429, 702, 3151, 4419, 25, 5798, 369, 8162, 21392, 11, 220, 16, 23, 5381, 4938, 11, 220, 16, 23, 23460, 22670, 11, 323, 220, 16, 31160, 36362, 382, 5338, 11, 358, 1184, 311, 19091, 279, 14260, 21219, 369, 57579, 31034, 13, 576, 386, 18, 1298, 1555, 5990, 8471, 518, 2163, 400, 16, 17, 24, 24, 21389, 16, 20, 24, 24, 438, 315, 4124, 220, 17, 15, 17, 19, 13, 1096, 6546, 4977, 1550, 13068, 481, 8162, 21392, 13230, 3162, 81178, 429, 2578, 387, 5802, 4960, 2783, 11, 5519, 220, 16, 23, 5381, 22038, 323, 458, 220, 16, 23, 23460, 22670, 525, 14848, 32247, 382, 785, 2331, 1614, 57579, 1298, 220, 16, 19, 1, 448, 386, 18, 1298, 8471, 2163, 400, 16, 17, 24, 24, 369, 279, 13420, 52839, 2635, 2319, 11, 714, 1052, 594, 1083, 264, 36843, 52433, 2999, 13, 576, 1196, 3207, 944, 13837, 892, 1614, 3498, 13, 2411, 419, 3349, 1459, 1711, 16, 19, 24, 24, 13, 24, 24, 701, 432, 2578, 387, 10078, 11392, 7707, 311, 1045, 32383, 11, 714, 537, 14312, 4969, 382, 82796, 4428, 47885, 448, 29039, 32247, 504, 1008, 15721, 1035, 2783, 803, 1091, 400, 16, 20, 15, 15, 369, 1493, 28703, 13657, 13, 8162, 594, 21219, 8282, 3545, 9892, 862, 1550, 57794, 4119, 518, 14848, 7576, 8818, 36571, 315, 5452, 13, 576, 3349, 1410, 1083, 8708, 7199, 18048, 476, 3151, 3081, 4682, 2337, 7050, 18346, 382, 27489, 11, 1393, 432, 2578, 2803, 264, 2699, 31266, 311, 1045, 22630, 7707, 311, 2331, 1614, 2606, 11, 419, 374, 2058, 2878, 279, 14260, 2088, 369, 7401, 24341, 448, 1493, 28703, 624, 151668, 198, 6555, 5868, 4622]

# Decode tokens to readable text
decoded_text = llm.detokenize(context_tokens).decode("utf-8")

# Inspect
print("üî¢ Token count:", len(context_tokens))
print("üì¶ Byte size:", len(decoded_text.encode("utf-8")))
print("üß† Decoded text:\n", decoded_text)

# Optional: trim to max token limit
MAX_TOKENS = 128000
if len(context_tokens) > MAX_TOKENS:
    trimmed = context_tokens[-MAX_TOKENS:]  # keep most recent
    print(f"\n‚úÇÔ∏è Trimmed to last {MAX_TOKENS} tokens.")
